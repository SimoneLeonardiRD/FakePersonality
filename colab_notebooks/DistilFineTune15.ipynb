{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DistilFineTune15.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46FV-73iST3v"
      },
      "source": [
        "# Fine Tuning DistilBert for BHV and Big5 Regression\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_RljvqsUtcP"
      },
      "source": [
        "## Environment setup "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIbHtcv9TCuV"
      },
      "source": [
        "Check if GPU is enabled in this environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNxjqFRFmGVg"
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RcSgoBfTUUf"
      },
      "source": [
        "Install HuggingFace transformer library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezCdYws_mUAW"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ja4H1hITtry"
      },
      "source": [
        "Mount Google Drive Repository for storage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7mf0xZ1szvb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBqTyUnJUFsT"
      },
      "source": [
        "Check if the Google Drive Folder is mounted correctly and undesrtand the path to be called"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7CYOt4SUT7I"
      },
      "source": [
        "!ls \"/content/drive/MyDrive/Colab Notebooks\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzblY_YfUlGy"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01mkrUikVCnl"
      },
      "source": [
        "Load a dataset made of sentences and labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcRytlrQVM9L"
      },
      "source": [
        "import pandas as pd\n",
        "pData = \"drive/MyDrive/Colab Notebooks/dataset/\"\n",
        "pModels = \"drive/MyDrive/Colab Notebooks/models/\"\n",
        "# df_text = pd.read_csv(pData + \"Big5/statuses_unicode.txt\",\n",
        "#                       header=None, names=['text'])\n",
        "df_text = pd.read_csv(pData + \"all_tweets_text.csv\")\n",
        "# df_labels = pd.read_csv(pData + \"Big5/big5labels.txt\",\n",
        "#                         delimiter=\" \", header=None,\n",
        "#                         names=['O', 'C', 'E', 'A', 'N'])\n",
        "df_labels = pd.read_csv(\n",
        "    pData + \"BHV/glove_all_bhv_all_tweets.csv\",\n",
        "    header=None,\n",
        "    names=['SD', 'ST', 'HE', 'AC', 'PO', 'SE', 'CO', 'TR', 'BE', 'UN'])\n",
        "df = pd.concat([df_text, df_labels], axis=1, sort=False)\n",
        "# df = df[:32]\n",
        "print(\"data shape\", df.shape)\n",
        "print(df.sample(2))\n",
        "df['text']= df['text'].astype('str')\n",
        "sentences  = df.text.values\n",
        "labels = df.UN.values  # <--- choose the one you need\n",
        "output_model_name = pModels+\"distil_UN\"  # <--- here too \n",
        "print(sentences[0], labels[0])\n",
        "print(sentences.shape, labels.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oMLcPc0g_q7"
      },
      "source": [
        "Labels do not need further processing so transform them into a tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cusn1sWxhHFA"
      },
      "source": [
        "labels = torch.tensor(labels)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3FAocdLbAGv"
      },
      "source": [
        "## Setup DistilBertForSequenceClassification "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCq5ckg0bJoh"
      },
      "source": [
        "Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiXPiCxTbNZR"
      },
      "source": [
        "from transformers import DistilBertTokenizerFast\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(\n",
        "    'distilbert-base-multilingual-cased',\n",
        "    do_lower_case=False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmSxzdt-bwvq"
      },
      "source": [
        "Check tokenizer effectiveness"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96DPOgivpYuZ"
      },
      "source": [
        "print(' Original: ', sentences[0])\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(\n",
        "      tokenizer.tokenize(sentences[0])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BynARNaacFBq"
      },
      "source": [
        "Find Max Sequence Length to avoid resource wasting "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uYP9GSZcRt7"
      },
      "source": [
        "max_len = 0\n",
        "for sent in sentences:\n",
        "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "print('Max sentence length: ', max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzZDpCExcnuo"
      },
      "source": [
        "Preprocess the sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7ZwcerzcrFr"
      },
      "source": [
        "input_ids = []\n",
        "attention_masks = []\n",
        "for sent in sentences:\n",
        "  encoded_dict = tokenizer.encode_plus(\n",
        "      sent,\n",
        "      add_special_tokens = True,\n",
        "      max_length = max_len,\n",
        "      # pad_to_max_length = True,\n",
        "      padding = 'max_length',\n",
        "      return_attention_mask = True,\n",
        "      return_tensors = 'pt'\n",
        "  )\n",
        "  input_ids.append(encoded_dict['input_ids'])\n",
        "  attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tcsw7GNuh0BL"
      },
      "source": [
        "Prepare the train_and_test TensorDataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3BOOaZAsVqI",
        "cellView": "code"
      },
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "# Combine the training inputs into a TensorDataset.\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "# Create a 90-10 train-validation split.\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1fu3E2FspJN"
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "batch_size = 32  # recommended 16 or 32\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    sampler = RandomSampler(train_dataset),\n",
        "    batch_size = batch_size\n",
        ")\n",
        "validation_dataloader = DataLoader(\n",
        "    val_dataset,\n",
        "    sampler = SequentialSampler(val_dataset),\n",
        "    batch_size = batch_size\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnzpMj2wjJ04"
      },
      "source": [
        "Select the model and its parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8_7sO6CjRs9"
      },
      "source": [
        "from transformers import DistilBertForSequenceClassification\n",
        "from transformers import DistilBertConfig, AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-multilingual-cased\",\n",
        "    num_labels = 1,  # <--- regression task\n",
        "    output_hidden_states = False,\n",
        "    output_attentions = False \n",
        ")\n",
        "model.cuda()  # <--- run this model on GPU\n",
        "\n",
        "optimizer = AdamW(\n",
        "    model.parameters(),\n",
        "    lr = 5e-5,\n",
        "    eps = 1e-8,\n",
        "    correct_bias=True\n",
        ")\n",
        "\n",
        "epochs = 2  # authors recommend between 2 and 4\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=len(train_dataloader)*epochs\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "143VenJ7mpOd"
      },
      "source": [
        "Define the function to compute time elapsed in each batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3J9HZMHVm1AP"
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlghw0-hnASR"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4En0AgXoUq9"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "seed_val = 1\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "training_stats = []\n",
        "total_t0 = time.time()\n",
        "\n",
        "for epoch_i in range(0, epochs):\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "    t0 = time.time()\n",
        "    total_train_loss = 0\n",
        "    total_eval_loss = 0\n",
        "    model.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print(step, len(train_dataloader), elapsed)\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].float().to(device)\n",
        "        model.zero_grad()\n",
        "        outputs = model(\n",
        "         b_input_ids,\n",
        "         attention_mask=b_input_mask,\n",
        "         labels=b_labels\n",
        "        )\n",
        "        total_train_loss += outputs.loss.item()\n",
        "        outputs.loss.type(torch.FloatTensor).backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "    training_time = format_time(time.time() - t0)\n",
        "    print(\"  Average training loss: {0:.8f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(training_time))\n",
        "\n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, \n",
        "    # measure our performance on our validation set.\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "    model.eval()\n",
        "    eval_mse,nb_eval_steps = 0, 0\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].float().to(device)\n",
        "        with torch.no_grad(): \n",
        "            outputs = model(\n",
        "             b_input_ids,\n",
        "             attention_mask=b_input_mask, \n",
        "             labels=b_labels\n",
        "            )\n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += outputs.loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = outputs.logits.detach().cpu().numpy()\n",
        "        #print(\"logits\", logits)\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        #print(\"labels\", label_ids)\n",
        "        pred_flat = logits.flatten()\n",
        "        labels_flat = label_ids.flatten()\n",
        "        tmp_eval_mse = mean_squared_error(pred_flat, labels_flat)\n",
        "        #tmp_eval_mcc_accuracy = matthews_corrcoef(labels_flat, pred_flat)\n",
        "      \n",
        "        eval_mse += tmp_eval_mse\n",
        "        #eval_mcc_accuracy += tmp_eval_mcc_accuracy\n",
        "        nb_eval_steps += 1\n",
        "    print(F'\\n\\tValidation mse: {eval_mse/nb_eval_steps}')\n",
        "model.save_pretrained(output_model_name)\n",
        "print(\"Training complete!\")\n",
        "print(\"Total {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vwN_3XdH_Jr"
      },
      "source": [
        "# loaded_model = DistilBertForSequenceClassification.from_pretrained(\n",
        "#     pModels+\"distil_O\", output_hidden_states = True)\n",
        "# sent1 = \"With their homes in ashes, residents share harrowing tales of survival after massive wildfires kill 15\"\n",
        "# sent2 = \"News anchor hits back at viewer who sent her snarky note about ‘showing too much cleavage’ during broadcast\"\n",
        "# max_len = 256 # the closest power of two exceeding max len found\n",
        "# input_ids = []\n",
        "# attention_masks = []\n",
        "# for sent in [sent1, sent2]:\n",
        "#   encoded_dict = tokenizer.encode_plus(\n",
        "#       sent,\n",
        "#       add_special_tokens = True,\n",
        "#       max_length = max_len,\n",
        "#       # pad_to_max_length = True,\n",
        "#       padding = 'max_length',\n",
        "#       return_attention_mask = True,\n",
        "#       return_tensors = 'pt'\n",
        "#   )\n",
        "#   # Add the encoded sentence to the list.    \n",
        "#   input_ids.append(encoded_dict['input_ids'])\n",
        "      \n",
        "#   # And its attention mask (simply differentiates padding from non-padding).\n",
        "#   attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# input_ids = torch.cat(input_ids, dim=0)\n",
        "# attention_masks = torch.cat(attention_masks, dim=0)\n",
        "# pt_output = loaded_model(input_ids, \n",
        "#                         attention_mask=attention_masks)\n",
        "\n",
        "# token_embeddings = torch.stack(pt_output.hidden_states, dim=0)\n",
        "# # print(token_embeddings.size())\n",
        "# last_layer = token_embeddings[-1]\n",
        "# # print(last_layer.size())\n",
        "# last_layer = last_layer.permute(1,0,2)\n",
        "# # print(last_layer[0].size()) # CLS token\n",
        "# #print(pt_output.hidden_states[-1].detach().numpy())\n",
        "# #print(pt_output.hidden_states[-1].detach().numpy().shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}