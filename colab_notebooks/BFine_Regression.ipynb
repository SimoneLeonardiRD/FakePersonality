{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "BFine_Regression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mecfnNxgrd6K"
      },
      "source": [
        "## Fine Tuning BERT for Regression Tasks\n",
        "\n",
        "1. [Fine-tune BERT and for regression problem](https://discuss.huggingface.co/t/fine-tune-bert-and-camembert-for-regression-problem/332)\n",
        "2. [Modify BertForSequenceClassification](https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bert.py#L1227)\n",
        "3. [Understand fine tuning](https://medium.com/@prakashakshay90/fine-tuning-bert-model-using-pytorch-f34148d58a37) phase with myPersonality"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4k5GtmwOlhRu",
        "outputId": "5e0cface-f16e-4950-e7ef-79ed7c2fd7cb"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNxjqFRFmGVg",
        "outputId": "d6010c35-8889-4e1d-82a2-4f9bf6c3c7e0"
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezCdYws_mUAW"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7mf0xZ1szvb",
        "outputId": "012c75bd-df3e-403b-89b0-6242590d1ddc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rn-XNr-Rrd6S"
      },
      "source": [
        "### Read myPersonality dataset status and big5 scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzG4CKRyuv4f"
      },
      "source": [
        "# !ls \"/content/drive/MyDrive\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gY97obnrd6T"
      },
      "source": [
        "import pandas as pd\n",
        "# Reading Data into dataFrame\n",
        "text = pd.read_csv(\"drive/MyDrive/Colab Notebooks/dataset/statuses_unicode.txt\", header=None, names=['sentence'])\n",
        "big5 = pd.read_csv(\"drive/MyDrive/Colab Notebooks/dataset/big5labels.txt\", delimiter=\" \", header=None, names=['O', 'C', 'E', 'A', 'N'])\n",
        "df = pd.concat([text, big5], axis=1, sort=False)\n",
        "#df = df[:32]\n",
        "print(df.shape)\n",
        "print(df.sample(5))\n",
        "df['sentence']= df['sentence'].astype('str')\n",
        "sentences = df.sentence.values\n",
        "labels = df.N.values # working with openness\n",
        "output_model_name = \"drive/MyDrive/Colab Notebooks/models/myPers_bert_fine_tuned_N\"\n",
        "print(sentences[8], labels[8])\n",
        "print(sentences.shape, labels.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kzze6DSKoYAi"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\n",
        "                'bert-base-multilingual-cased',\n",
        "                do_lower_case=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96DPOgivpYuZ"
      },
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', sentences[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RmZIJ9-pvzU"
      },
      "source": [
        "max_len = 0\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "\n",
        "    # Update the maximum sentence length.\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "\n",
        "print('Max sentence length: ', max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuWnE51mrd6a"
      },
      "source": [
        "attention mask"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97IxqFfHqBEm"
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "max_len = 256 # the closest power of two exceeding max len found\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = max_len,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3BOOaZAsVqI",
        "cellView": "code"
      },
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "# Combine the training inputs into a TensorDataset.\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "# Create a 90-10 train-validation split.\n",
        "\n",
        "# Calculate the number of samples to include in each set.\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1fu3E2FspJN"
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
        "# size of 16 or 32.\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zMD85Card6e"
      },
      "source": [
        "### Train the model changing the task from classification to regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0J38EMyArd6e"
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-multilingual-cased\",\n",
        "    num_labels=1, #configure the model to perform regression and change the loss into Mean-Square Loss\n",
        "    output_hidden_states = False, # prendiamo solo l'ultimo layer\n",
        "    output_attentions = False\n",
        ") # output_hidden_states ci permette di estrarre gli embeddings\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuhMJ9J4tR2D"
      },
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72qX00sTrd6f"
      },
      "source": [
        "# Parameters:\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 3e-5, # args.learning_rate - default is 5e-5,\n",
        "                  eps = 1e-8, # args.adam_epsilon  - default is 1e-8.\n",
        "                  correct_bias=True # To reproduce BertAdam specific behavior set correct_bias=False\n",
        "                )\n",
        "\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 10\n",
        "\n",
        "num_warmup_steps = 0\n",
        "num_training_steps = len(train_dataloader)*epochs\n",
        "\n",
        "#Prepare optimizer and schedule (linear warmup and decay)\n",
        "# no_decay = ['bias', 'LayerNorm.weight']\n",
        "# optimizer_grouped_parameters = [\n",
        "#     {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "#     {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "#     ]\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps=num_warmup_steps,\n",
        "                                            num_training_steps=num_training_steps)  # PyTorch scheduler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axf96ySmtsXl"
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnS6VRaayMp_"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOL9uRyLrd6g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5915c800-b790-4ca6-c58b-2a6b2faf36f4"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.metrics import mean_squared_error\n",
        "seed_val = 1 \n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "training_stats = []\n",
        "total_t0 = time.time()\n",
        "for epoch_i in range(0, epochs):\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "    t0 = time.time()\n",
        "    total_train_loss = 0\n",
        "    total_eval_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].float().to(device)\n",
        "        model.zero_grad()\n",
        "        outputs = model(b_input_ids, \n",
        "                             token_type_ids=None, \n",
        "                             attention_mask=b_input_mask, \n",
        "                             labels=b_labels)\n",
        "        total_train_loss += outputs.loss.item()\n",
        "        # print(\"train b_labels\", b_labels)\n",
        "        # print(\"train logits\", outputs.logits)\n",
        "\n",
        "        outputs.loss.type(torch.FloatTensor).backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(training_time))\n",
        "\n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "    model.eval()\n",
        "    eval_mse,nb_eval_steps = 0, 0\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].float().to(device)\n",
        "        with torch.no_grad(): \n",
        "            outputs = model(b_input_ids, \n",
        "                                   token_type_ids=None, \n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += outputs.loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = outputs.logits.detach().cpu().numpy()\n",
        "        #print(\"logits\", logits)\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        #print(\"labels\", label_ids)\n",
        "        pred_flat = logits.flatten()\n",
        "        labels_flat = label_ids.flatten()\n",
        "        tmp_eval_mse = mean_squared_error(pred_flat, labels_flat)\n",
        "        #tmp_eval_mcc_accuracy = matthews_corrcoef(labels_flat, pred_flat)\n",
        "      \n",
        "        eval_mse += tmp_eval_mse\n",
        "        #eval_mcc_accuracy += tmp_eval_mcc_accuracy\n",
        "        nb_eval_steps += 1\n",
        "    print(F'\\n\\tValidation mse: {eval_mse/nb_eval_steps}')\n",
        "    #print(F'\\n\\tValidation MCC Accuracy: {eval_mcc_accuracy/nb_eval_steps}')\n",
        "    print(\"pred\", pred_flat)\n",
        "    print(\"original\", labels_flat)\n",
        "    \n",
        "model.save_pretrained(output_model_name)\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    279.    Elapsed: 0:00:31.\n",
            "  Batch    80  of    279.    Elapsed: 0:01:02.\n",
            "  Batch   120  of    279.    Elapsed: 0:01:34.\n",
            "  Batch   160  of    279.    Elapsed: 0:02:05.\n",
            "  Batch   200  of    279.    Elapsed: 0:02:36.\n",
            "  Batch   240  of    279.    Elapsed: 0:03:07.\n",
            "\n",
            "  Average training loss: 0.66\n",
            "  Training epoch took: 0:03:38\n",
            "\n",
            "Running Validation...\n",
            "\n",
            "\tValidation mse: 0.5825167663635746\n",
            "pred [2.6716192 2.6035268 2.6129446 2.7873814 2.7606938 2.6426182 2.623395\n",
            " 2.5436513 2.7837799 2.8264434 2.7294607 2.6869934 2.6101263 2.56546\n",
            " 2.7194693 2.7366698 2.6078482 2.6718383 2.5820603 2.7374005 2.5228782\n",
            " 2.7152436 2.5824583 2.6417053 2.718999  2.7201421 2.6469526 2.7702997\n",
            " 2.6645272 2.6528344 2.6535451 2.6040194]\n",
            "original [1.5  2.85 3.85 3.05 2.9  3.25 4.75 3.75 2.5  2.9  2.8  1.43 3.75 2.3\n",
            " 4.25 1.9  1.43 2.7  3.7  2.65 2.25 2.45 1.43 3.5  2.3  2.85 2.8  3.75\n",
            " 2.25 2.45 3.3  1.75]\n",
            "\n",
            "======== Epoch 2 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    279.    Elapsed: 0:00:31.\n",
            "  Batch    80  of    279.    Elapsed: 0:01:02.\n",
            "  Batch   120  of    279.    Elapsed: 0:01:34.\n",
            "  Batch   160  of    279.    Elapsed: 0:02:05.\n",
            "  Batch   200  of    279.    Elapsed: 0:02:36.\n",
            "  Batch   240  of    279.    Elapsed: 0:03:07.\n",
            "\n",
            "  Average training loss: 0.57\n",
            "  Training epoch took: 0:03:37\n",
            "\n",
            "Running Validation...\n",
            "\n",
            "\tValidation mse: 0.5780608692476826\n",
            "pred [2.6476414 2.6137478 2.4693694 2.4513109 2.7231677 2.505211  2.6646233\n",
            " 2.4792593 2.7374773 2.7298837 2.7338715 2.5319533 2.5997224 2.469724\n",
            " 2.4343317 2.741572  2.6666353 2.7281582 2.6510212 2.732806  2.3405316\n",
            " 2.5789113 2.736439  2.735787  2.7287955 2.5132678 2.7253373 2.7308648\n",
            " 2.6477    2.476788  2.389771  2.6489804]\n",
            "original [1.5  2.85 3.85 3.05 2.9  3.25 4.75 3.75 2.5  2.9  2.8  1.43 3.75 2.3\n",
            " 4.25 1.9  1.43 2.7  3.7  2.65 2.25 2.45 1.43 3.5  2.3  2.85 2.8  3.75\n",
            " 2.25 2.45 3.3  1.75]\n",
            "\n",
            "======== Epoch 3 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    279.    Elapsed: 0:00:31.\n",
            "  Batch    80  of    279.    Elapsed: 0:01:02.\n",
            "  Batch   120  of    279.    Elapsed: 0:01:34.\n",
            "  Batch   160  of    279.    Elapsed: 0:02:05.\n",
            "  Batch   200  of    279.    Elapsed: 0:02:36.\n",
            "  Batch   240  of    279.    Elapsed: 0:03:07.\n",
            "\n",
            "  Average training loss: 0.50\n",
            "  Training epoch took: 0:03:37\n",
            "\n",
            "Running Validation...\n",
            "\n",
            "\tValidation mse: 0.639328965256291\n",
            "pred [2.7510514 2.7255325 2.613569  2.3410816 3.1917932 2.8458488 2.7837224\n",
            " 2.2996948 3.3225336 3.2136095 3.0666153 2.9701364 2.6895337 2.0785613\n",
            " 2.6649082 2.9332669 2.924818  3.308268  3.1816201 3.26331   2.3245282\n",
            " 2.7500422 3.1736183 2.667114  2.8081274 2.4918053 2.977101  3.062387\n",
            " 3.0365107 2.5031536 2.8469334 2.6597383]\n",
            "original [1.5  2.85 3.85 3.05 2.9  3.25 4.75 3.75 2.5  2.9  2.8  1.43 3.75 2.3\n",
            " 4.25 1.9  1.43 2.7  3.7  2.65 2.25 2.45 1.43 3.5  2.3  2.85 2.8  3.75\n",
            " 2.25 2.45 3.3  1.75]\n",
            "\n",
            "======== Epoch 4 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    279.    Elapsed: 0:00:31.\n",
            "  Batch    80  of    279.    Elapsed: 0:01:02.\n",
            "  Batch   120  of    279.    Elapsed: 0:01:34.\n",
            "  Batch   160  of    279.    Elapsed: 0:02:05.\n",
            "  Batch   200  of    279.    Elapsed: 0:02:36.\n",
            "  Batch   240  of    279.    Elapsed: 0:03:07.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0:03:38\n",
            "\n",
            "Running Validation...\n",
            "\n",
            "\tValidation mse: 0.6486969109504453\n",
            "pred [2.5151339 2.456798  2.5920305 2.184973  3.226351  2.721449  2.6838531\n",
            " 2.1543279 3.1566868 3.282639  3.1858938 2.8769662 2.591624  2.0877767\n",
            " 2.5082252 2.8067558 3.096816  3.5309844 3.2686288 3.3780859 2.3827636\n",
            " 2.7607806 3.8262444 2.7989664 2.409237  2.347007  3.089651  2.8906724\n",
            " 2.8617978 2.5212667 2.677646  2.4596562]\n",
            "original [1.5  2.85 3.85 3.05 2.9  3.25 4.75 3.75 2.5  2.9  2.8  1.43 3.75 2.3\n",
            " 4.25 1.9  1.43 2.7  3.7  2.65 2.25 2.45 1.43 3.5  2.3  2.85 2.8  3.75\n",
            " 2.25 2.45 3.3  1.75]\n",
            "\n",
            "======== Epoch 5 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    279.    Elapsed: 0:00:31.\n",
            "  Batch    80  of    279.    Elapsed: 0:01:02.\n",
            "  Batch   120  of    279.    Elapsed: 0:01:34.\n",
            "  Batch   160  of    279.    Elapsed: 0:02:05.\n",
            "  Batch   200  of    279.    Elapsed: 0:02:36.\n",
            "  Batch   240  of    279.    Elapsed: 0:03:07.\n",
            "\n",
            "  Average training loss: 0.22\n",
            "  Training epoch took: 0:03:38\n",
            "\n",
            "Running Validation...\n",
            "\n",
            "\tValidation mse: 0.6721775310654794\n",
            "pred [2.4683452 2.1902652 2.3975325 1.729285  2.963853  2.4064927 2.4389138\n",
            " 1.8613601 2.8526177 3.3683498 3.1945775 2.5911434 2.5074062 1.7793639\n",
            " 2.4464655 2.5788152 2.8826604 2.970877  3.1526031 2.9394536 2.1505415\n",
            " 2.7617385 3.3563251 2.607857  2.4582367 2.067511  3.0642986 2.6205232\n",
            " 2.2940788 2.3912432 2.0968938 2.3619215]\n",
            "original [1.5  2.85 3.85 3.05 2.9  3.25 4.75 3.75 2.5  2.9  2.8  1.43 3.75 2.3\n",
            " 4.25 1.9  1.43 2.7  3.7  2.65 2.25 2.45 1.43 3.5  2.3  2.85 2.8  3.75\n",
            " 2.25 2.45 3.3  1.75]\n",
            "\n",
            "======== Epoch 6 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    279.    Elapsed: 0:00:31.\n",
            "  Batch    80  of    279.    Elapsed: 0:01:02.\n",
            "  Batch   120  of    279.    Elapsed: 0:01:34.\n",
            "  Batch   160  of    279.    Elapsed: 0:02:05.\n",
            "  Batch   200  of    279.    Elapsed: 0:02:36.\n",
            "  Batch   240  of    279.    Elapsed: 0:03:07.\n",
            "\n",
            "  Average training loss: 0.15\n",
            "  Training epoch took: 0:03:37\n",
            "\n",
            "Running Validation...\n",
            "\n",
            "\tValidation mse: 0.6747116629154452\n",
            "pred [2.4319725 2.120398  2.6124802 1.7715548 3.0188873 2.424045  2.6353586\n",
            " 1.9199799 2.9718742 4.1236606 3.2970042 2.7608376 2.6561012 1.7399583\n",
            " 2.4098701 2.630718  3.0427997 2.9731162 3.0935903 2.8490756 2.1350322\n",
            " 2.9964821 3.0043066 2.4451172 2.4783108 2.4292908 3.111317  2.7469206\n",
            " 2.3415704 2.5076318 2.467658  2.599856 ]\n",
            "original [1.5  2.85 3.85 3.05 2.9  3.25 4.75 3.75 2.5  2.9  2.8  1.43 3.75 2.3\n",
            " 4.25 1.9  1.43 2.7  3.7  2.65 2.25 2.45 1.43 3.5  2.3  2.85 2.8  3.75\n",
            " 2.25 2.45 3.3  1.75]\n",
            "\n",
            "======== Epoch 7 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    279.    Elapsed: 0:00:31.\n",
            "  Batch    80  of    279.    Elapsed: 0:01:02.\n",
            "  Batch   120  of    279.    Elapsed: 0:01:33.\n",
            "  Batch   160  of    279.    Elapsed: 0:02:05.\n",
            "  Batch   200  of    279.    Elapsed: 0:02:36.\n",
            "  Batch   240  of    279.    Elapsed: 0:03:07.\n",
            "\n",
            "  Average training loss: 0.11\n",
            "  Training epoch took: 0:03:37\n",
            "\n",
            "Running Validation...\n",
            "\n",
            "\tValidation mse: 0.6788247708351381\n",
            "pred [2.6030202 2.3271756 2.5255744 1.7841263 3.1159368 2.6173186 2.7331128\n",
            " 1.8976306 3.071666  3.6637998 3.2073271 2.8543403 2.699125  2.093469\n",
            " 2.5478644 2.680818  3.1725142 3.017497  3.1489282 3.092423  2.2422678\n",
            " 2.8939967 3.0873249 2.5945234 2.5807376 2.477882  3.0347376 2.6936061\n",
            " 2.2960267 2.726195  2.7769682 2.628808 ]\n",
            "original [1.5  2.85 3.85 3.05 2.9  3.25 4.75 3.75 2.5  2.9  2.8  1.43 3.75 2.3\n",
            " 4.25 1.9  1.43 2.7  3.7  2.65 2.25 2.45 1.43 3.5  2.3  2.85 2.8  3.75\n",
            " 2.25 2.45 3.3  1.75]\n",
            "\n",
            "======== Epoch 8 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    279.    Elapsed: 0:00:31.\n",
            "  Batch    80  of    279.    Elapsed: 0:01:02.\n",
            "  Batch   120  of    279.    Elapsed: 0:01:33.\n",
            "  Batch   160  of    279.    Elapsed: 0:02:05.\n",
            "  Batch   200  of    279.    Elapsed: 0:02:36.\n",
            "  Batch   240  of    279.    Elapsed: 0:03:07.\n",
            "\n",
            "  Average training loss: 0.09\n",
            "  Training epoch took: 0:03:37\n",
            "\n",
            "Running Validation...\n",
            "\n",
            "\tValidation mse: 0.7020945779738887\n",
            "pred [2.6111293 2.1321697 2.3579335 1.7666795 3.0633392 2.477328  2.5853033\n",
            " 1.7471771 3.212074  3.4376302 3.1489687 2.8465219 2.754205  1.8337232\n",
            " 2.5080884 2.6871667 3.0339947 3.0277853 3.3302064 3.057344  2.2018971\n",
            " 2.929789  3.373343  2.5883918 2.5256784 2.2043903 3.0172842 2.6041086\n",
            " 2.1016593 2.6250236 2.7000372 2.6084797]\n",
            "original [1.5  2.85 3.85 3.05 2.9  3.25 4.75 3.75 2.5  2.9  2.8  1.43 3.75 2.3\n",
            " 4.25 1.9  1.43 2.7  3.7  2.65 2.25 2.45 1.43 3.5  2.3  2.85 2.8  3.75\n",
            " 2.25 2.45 3.3  1.75]\n",
            "\n",
            "======== Epoch 9 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    279.    Elapsed: 0:00:31.\n",
            "  Batch    80  of    279.    Elapsed: 0:01:02.\n",
            "  Batch   120  of    279.    Elapsed: 0:01:33.\n",
            "  Batch   160  of    279.    Elapsed: 0:02:05.\n",
            "  Batch   200  of    279.    Elapsed: 0:02:36.\n",
            "  Batch   240  of    279.    Elapsed: 0:03:07.\n",
            "\n",
            "  Average training loss: 0.07\n",
            "  Training epoch took: 0:03:37\n",
            "\n",
            "Running Validation...\n",
            "\n",
            "\tValidation mse: 0.6674518123749764\n",
            "pred [2.4076285 2.0783474 2.359836  1.7407159 2.85217   2.30561   2.5446246\n",
            " 1.8212861 3.0271108 3.8387516 2.9855804 2.6683044 2.618214  1.8413961\n",
            " 2.4018586 2.5644276 2.8471591 2.8925402 3.0369806 2.8992946 2.0921082\n",
            " 2.7502706 3.2329342 2.6009793 2.465547  2.1974545 2.9140296 2.397792\n",
            " 2.1411505 2.4910307 2.4507215 2.4629471]\n",
            "original [1.5  2.85 3.85 3.05 2.9  3.25 4.75 3.75 2.5  2.9  2.8  1.43 3.75 2.3\n",
            " 4.25 1.9  1.43 2.7  3.7  2.65 2.25 2.45 1.43 3.5  2.3  2.85 2.8  3.75\n",
            " 2.25 2.45 3.3  1.75]\n",
            "\n",
            "======== Epoch 10 / 10 ========\n",
            "Training...\n",
            "  Batch    40  of    279.    Elapsed: 0:00:31.\n",
            "  Batch    80  of    279.    Elapsed: 0:01:02.\n",
            "  Batch   120  of    279.    Elapsed: 0:01:34.\n",
            "  Batch   160  of    279.    Elapsed: 0:02:05.\n",
            "  Batch   200  of    279.    Elapsed: 0:02:36.\n",
            "  Batch   240  of    279.    Elapsed: 0:03:07.\n",
            "\n",
            "  Average training loss: 0.06\n",
            "  Training epoch took: 0:03:37\n",
            "\n",
            "Running Validation...\n",
            "\n",
            "\tValidation mse: 0.6729633279385105\n",
            "pred [2.4543703 2.0836875 2.3755372 1.6812224 2.9179847 2.3356092 2.59112\n",
            " 1.8429354 3.0807831 3.7730856 3.0790317 2.7677832 2.732681  1.8653184\n",
            " 2.4564574 2.7104213 2.9978747 2.9546838 3.10566   2.924516  2.099811\n",
            " 2.7729385 3.3162308 2.667838  2.582665  2.2552795 2.9835572 2.458871\n",
            " 2.202699  2.5166757 2.4927025 2.5537636]\n",
            "original [1.5  2.85 3.85 3.05 2.9  3.25 4.75 3.75 2.5  2.9  2.8  1.43 3.75 2.3\n",
            " 4.25 1.9  1.43 2.7  3.7  2.65 2.25 2.45 1.43 3.5  2.3  2.85 2.8  3.75\n",
            " 2.25 2.45 3.3  1.75]\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:37:36 (h:mm:ss)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4QXC-IwH807"
      },
      "source": [
        "load pretrained and finetuned"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "6vwN_3XdH_Jr",
        "outputId": "8d7795ea-e972-4c41-8a8c-b1a49fa8b57f"
      },
      "source": [
        "'''\n",
        "loaded_model = BertForSequenceClassification.from_pretrained(\"drive/MyDrive/Colab Notebooks/models/myPers_bert_fine_tuned_O\", output_hidden_states = True)\n",
        "sent1 = \"With their homes in ashes, residents share harrowing tales of survival after massive wildfires kill 15\"\n",
        "sent2 = \"News anchor hits back at viewer who sent her snarky note about ‘showing too much cleavage’ during broadcast\"\n",
        "max_len = 256 # the closest power of two exceeding max len found\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "for sent in [sent1, sent2]:\n",
        "  encoded_dict = tokenizer.encode_plus(\n",
        "                          sent,                      # Sentence to encode.\n",
        "                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                          max_length = max_len,           # Pad & truncate all sentences.\n",
        "                          pad_to_max_length = True,\n",
        "                          return_attention_mask = True,   # Construct attn. masks.\n",
        "                          return_tensors = 'pt'     # Return pytorch tensors.\n",
        "                    )\n",
        "  # Add the encoded sentence to the list.    \n",
        "  input_ids.append(encoded_dict['input_ids'])\n",
        "      \n",
        "  # And its attention mask (simply differentiates padding from non-padding).\n",
        "  attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "pt_output = loaded_model(input_ids, token_type_ids=None, \n",
        "                        attention_mask=attention_masks)\n",
        "\n",
        "token_embeddings = torch.stack(pt_output.hidden_states, dim=0)\n",
        "print(token_embeddings.size())\n",
        "last_layer = token_embeddings[-1]\n",
        "print(last_layer.size())\n",
        "last_layer = last_layer.permute(1,0,2)\n",
        "print(last_layer[0].size()) # CLS token\n",
        "\n",
        "# print(pt_output.hidden_states[-1].detach().numpy())\n",
        "# print(pt_output.hidden_states[-1].detach().numpy().shape)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nloaded_model = BertForSequenceClassification.from_pretrained(\"drive/MyDrive/Colab Notebooks/models/myPers_bert_fine_tuned_O\", output_hidden_states = True)\\nsent1 = \"With their homes in ashes, residents share harrowing tales of survival after massive wildfires kill 15\"\\nsent2 = \"News anchor hits back at viewer who sent her snarky note about ‘showing too much cleavage’ during broadcast\"\\nmax_len = 256 # the closest power of two exceeding max len found\\ninput_ids = []\\nattention_masks = []\\nfor sent in [sent1, sent2]:\\n  encoded_dict = tokenizer.encode_plus(\\n                          sent,                      # Sentence to encode.\\n                          add_special_tokens = True, # Add \\'[CLS]\\' and \\'[SEP]\\'\\n                          max_length = max_len,           # Pad & truncate all sentences.\\n                          pad_to_max_length = True,\\n                          return_attention_mask = True,   # Construct attn. masks.\\n                          return_tensors = \\'pt\\'     # Return pytorch tensors.\\n                    )\\n  # Add the encoded sentence to the list.    \\n  input_ids.append(encoded_dict[\\'input_ids\\'])\\n      \\n  # And its attention mask (simply differentiates padding from non-padding).\\n  attention_masks.append(encoded_dict[\\'attention_mask\\'])\\n\\ninput_ids = torch.cat(input_ids, dim=0)\\nattention_masks = torch.cat(attention_masks, dim=0)\\npt_output = loaded_model(input_ids, token_type_ids=None, \\n                        attention_mask=attention_masks)\\n\\ntoken_embeddings = torch.stack(pt_output.hidden_states, dim=0)\\nprint(token_embeddings.size())\\nlast_layer = token_embeddings[-1]\\nprint(last_layer.size())\\nlast_layer = last_layer.permute(1,0,2)\\nprint(last_layer[0].size()) # CLS token\\n\\n# print(pt_output.hidden_states[-1].detach().numpy())\\n# print(pt_output.hidden_states[-1].detach().numpy().shape)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    }
  ]
}