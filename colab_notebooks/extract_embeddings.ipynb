{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"extract_embeddings.ipynb","private_outputs":true,"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"4G-xQIODTer5"},"source":["## Sentence/Word Embeddings Ectraction for downline activities\n"]},{"cell_type":"code","metadata":{"id":"uNxjqFRFmGVg"},"source":["import torch\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ezCdYws_mUAW"},"source":["!pip install transformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K7mf0xZ1szvb"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JBW4m7nybiVs"},"source":["\n","import pandas as pd\n","df = pd.read_csv(\"drive/MyDrive/Colab Notebooks/dataset/all_tweets_text.csv\")\n","print(df.shape)\n","print(df.columns)\n","df['text']= df['text'].astype('str')\n","#df = df[8242:]\n","#df = df[:37]\n","sentences = df.text.values\n","print(sentences[8])\n","print(sentences.shape)\n","\n","'''\n","import pandas as pd\n","# Reading Data into dataFrame\n","text = pd.read_csv(\"drive/MyDrive/Colab Notebooks/dataset/statuses_unicode.txt\", header=None, names=['sentence'])\n","big5 = pd.read_csv(\"drive/MyDrive/Colab Notebooks/dataset/big5labels.txt\", delimiter=\" \", header=None, names=['O', 'C', 'E', 'A', 'N'])\n","df = pd.concat([text, big5], axis=1, sort=False)\n","#df = df[:32]\n","print(df.shape)\n","print(df.sample(5))\n","df['sentence']= df['sentence'].astype('str')\n","sentences = df.sentence.values\n","print(sentences[8])\n","print(sentences.shape)\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kzze6DSKoYAi"},"source":["from transformers import BertTokenizer\n","tokenizer = BertTokenizer.from_pretrained(\n","                'bert-base-multilingual-cased',\n","                do_lower_case=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"96DPOgivpYuZ"},"source":["# Print the original sentence.\n","print(' Original: ', sentences[0])\n","\n","# Print the sentence split into tokens.\n","print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n","\n","# Print the sentence mapped to token ids.\n","print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0RmZIJ9-pvzU"},"source":["max_len = 0\n","\n","# For every sentence...\n","for sent in sentences:\n","    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n","    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n","\n","    # Update the maximum sentence length.\n","    max_len = max(max_len, len(input_ids))\n","\n","print('Max sentence length: ', max_len)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"97IxqFfHqBEm"},"source":["# Tokenize all of the sentences and map the tokens to thier word IDs.\n","max_len = 256 # the closest power of two exceeding max len found\n","input_ids = []\n","attention_masks = []\n","\n","# For every sentence...\n","for sent in sentences:\n","    # `encode_plus` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    #   (5) Pad or truncate the sentence to `max_length`\n","    #   (6) Create attention masks for [PAD] tokens.\n","    encoded_dict = tokenizer.encode_plus(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = max_len,           # Pad & truncate all sentences.\n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","    \n","    # Add the encoded sentence to the list.    \n","    input_ids.append(encoded_dict['input_ids'])\n","    \n","    # And its attention mask (simply differentiates padding from non-padding).\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","# Convert the lists into tensors.\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","\n","# Print sentence 0, now as a list of IDs.\n","print('Original: ', sentences[0])\n","print('Token IDs:', input_ids[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DNUPruYheNaw","cellView":"code"},"source":["#@title Testo del titolo predefinito\n","from transformers import BertModel\n","model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n","model.cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N1BxZu-lfG1U"},"source":["import numpy as np\n","import sys\n","from scipy.spatial import distance\n","\n","bhv_centroids = np.load(\"drive/MyDrive/Colab Notebooks/dataset/bhv_centroids.npy\")\n","\n","batch_size = 16\n","start_range = 0\n","print(input_ids.shape[0])\n","bhv_results = []\n","\n","while (start_range + batch_size) < input_ids.shape[0]:\n","  inputs = input_ids[start_range: (start_range + batch_size)]\n","  outputs = model(inputs.to(device))\n","  #print(outputs.last_hidden_state.shape)\n","  #calcola bhv qui\n","  out_arr = outputs.last_hidden_state.cpu().detach().numpy()\n","  out_arr_asa = np.asarray(out_arr)\n","\n","  \n","  for sentence in out_arr_asa:\n","      valid_token = 0.0\n","      bhv_scores = np.zeros(10)\n","      for token in sentence:\n","          min_distance = sys.float_info.max\n","          pos = 0\n","          if abs(token[0]) < 0.0000001:\n","              continue\n","          else:\n","              for i in range(10):\n","                  now_distance = distance.euclidean(np.asarray(token), np.asarray(bhv_centroids[i]))\n","                  #print(now_distance)\n","                  if now_distance < min_distance:\n","                      min_distance = now_distance\n","                      pos = i\n","              bhv_scores[pos] = bhv_scores[pos]+1.0\n","              valid_token = valid_token + 1.0\n","      #print(\"bhv_scores and valid token\", bhv_scores, valid_token)\n","      bhv_scores = bhv_scores / valid_token\n","      #print(\"bhv_scores after div\", bhv_scores)\n","      bhv_results.append(bhv_scores)\n","\n","  start_range += batch_size\n","\n","\n","inputs = input_ids[start_range:]\n","outputs = model(inputs.to(device))\n","#calcola bhv anche qui\n","out_arr = outputs.last_hidden_state.cpu().detach().numpy()\n","out_arr_asa = np.asarray(out_arr)\n","\n","for sentence in out_arr_asa:\n","    valid_token = 0.0\n","    bhv_scores = np.zeros(10)\n","    for token in sentence:\n","        min_distance = sys.float_info.max\n","        pos = 0\n","        if abs(token[0]) < 0.0000001:\n","            continue\n","        else:\n","            for i in range(10):\n","                now_distance = distance.euclidean(np.asarray(token), np.asarray(bhv_centroids[i]))\n","                #print(now_distance)\n","                if now_distance < min_distance:\n","                    min_distance = now_distance\n","                    pos = i\n","            bhv_scores[pos] = bhv_scores[pos]+1.0\n","            valid_token = valid_token + 1.0\n","    bhv_scores = bhv_scores / valid_token\n","    bhv_results.append(bhv_scores)\n","\n","\n","# print(\"bhv_results \", bhv_results)\n","bhv_res = np.asarray(bhv_results)\n","np.savetxt(\"drive/MyDrive/Colab Notebooks/dataset/BHV/all_bhv.csv\", bhv_res, delimiter=\",\", fmt='%5.5f')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rgp7YToB7Zw-"},"source":["# BHV with GloVe "]},{"cell_type":"code","metadata":{"id":"Pt9VFwc_9YRv"},"source":["!wget http://nlp.stanford.edu/data/glove.twitter.27B.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nmNs9S_e-CET"},"source":["!unzip -q glove.twitter.27B.zip -d 'drive/MyDrive/Colab Notebooks/dataset/GloVe'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MiZn544pAxNi"},"source":["import numpy as np\n","\n","def loadGloveModel(gloveFile):     ##200d\n","    print (\"Loading Glove Model\")\n","    f = open(gloveFile,'r')\n","    model = {}\n","    for line in f:\n","        splitLine = line.split()\n","        word = splitLine[0]\n","        embedding = np.array([float(val) for val in splitLine[1:]])\n","        model[word] = embedding\n","    print (\"Done.\",len(model),\" words loaded!\")\n","    return model\n","localModel = loadGloveModel('drive/MyDrive/Colab Notebooks/dataset/GloVe/glove.twitter.27B.200d.txt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EPUH8v13B-7m"},"source":["selfdirection = [\"creativity\", \"freedom\", \"curious\", \"independent\", \"self-respect\", \"intelligent\", \"privacy\"]\n","stimulation = [\"excitement\", \"novelty\", \"challenge\", \"variety\", \"stimulation\", \"daring\"]\n","hedonism = [\"pleasure\", \"sensuous\",  \"gratification\", \"enjoyable\", \"self-indulgent\"]\n","achievement = [\"ambitious\", \"successful\", \"capable\", \"influential\", \"intelligent\", \"self-respect\"] \n","power = [\"authority\", \"wealth\", \"power\", \"reputation\", \"notoriety\"]\n","security = [\"safety\", \"harmony\", \"stability\", \"order\", \"security\", \"clean\", \"reciprocation\", \"healthy\", \"moderate\", \"belonging\"]\n","conformity = [\"obedient\", \"self-discipline\", \"politeness\", \"honoring\" , \"loyal\", \"responsible\"]\n","tradition = [\"tradition\", \"humble\", \"devout\", \"moderate\", \"spiritualist\"]\n","benevolence = [\"helpful\", \"honest\", \"forgiving\", \"responsible\", \"loyal\", \"friendship\", \"love\", \"meaningful\"]\n","universalism = [\"broadminded\", \"justice\", \"equality\", \"peace\", \"beauty\", \"wisdom\", \"environmentalist\", \"harmony\"]\n","\n","schwartzBasicHumanValues = [selfdirection, stimulation, hedonism, achievement, power, security, conformity, tradition, benevolence, universalism]\n","schwartzNames = [\"selfdirection\", \"stimulation\", \"hedonism\", \"achievement\", \"power\", \"security\", \"conformity\", \"tradition\", \"benevolence\", \"universalism\"]\n","\n","pos = 0\n","schwartzCentroids = {}\n","\n","for humanValue in schwartzBasicHumanValues:\n","\tcount_elements = 0.0\n","\tschwartzNCentroid = [0.0]\n","\tschwartzNCentroid = schwartzNCentroid*200\n","\tschwartzNCentroid = np.asarray(schwartzNCentroid)\n","\tfor representativeWord in humanValue:\n","\t\tschwartzNCentroid = schwartzNCentroid + np.asarray(localModel[representativeWord])\n","\t\tcount_elements +=1\n","\tschwartzCentroids[schwartzNames[pos]] = schwartzNCentroid/count_elements\n","\tpos +=1\n","print (\"Centroids computed!\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zc6A1DBeMMHS"},"source":["nltk.download('stopwords')\n","nltk.download('wordnet')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qSRfJPMiDEiu"},"source":["import nltk\n","\n","from nltk.corpus import stopwords \n","import string\n","from nltk.stem.wordnet import WordNetLemmatizer\n","import re\n","\n","stop = set(stopwords.words('english'))\n","exclude = set(string.punctuation) \n","lemma = WordNetLemmatizer()\n","def clean(doc):\n","    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n","    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n","    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n","    return normalized\n","\n","NON_BMP_RE = re.compile(u\"[^\\U00000000-\\U0000d7ff\\U0000e000-\\U0000ffff]\", flags=re.UNICODE)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YE5yTH-Q7kWw"},"source":["import sys\n","from scipy.spatial import distance\n","\n","bhv_results = []\n","\n","for sentence in sentences:\n","  total_words = {}\n","  words_total = 0.0\n","  bhv_scores = np.zeros(10)\n","  for category in schwartzNames:\n","    total_words[category] = 0\n","\n","  \n","  doc_cleaned = clean(sentence)\n","  # print(doc_cleaned)\n","  for word in doc_cleaned:\n","    if word.startswith('@') or word.isdigit() or (\"http\" in word):\n","      continue\n","    else:\n","      word = NON_BMP_RE.sub('', word)\n","      if len(word)>0 and word in localModel:\n","        words_total += 1\n","        min_distance = sys.float_info.max\n","        which_schwartz = \"\"\n","        for pos in schwartzNames:\n","          now_distance = distance.euclidean(np.asarray(localModel[word]), schwartzCentroids[pos])\n","          if now_distance<min_distance:\n","            min_distance = now_distance\n","            which_schwartz = pos\n","        total_words[which_schwartz] += 1\n","  pos = 0\n","  if words_total == 0.0:\n","    bhv_results.append(bhv_scores)\n","  else:\n","    for category in schwartzNames:\n","      bhv_scores[pos] = total_words[category]/words_total\n","      pos +=1\n","    bhv_results.append(bhv_scores)\n","  \n","print (\"bhv computed successfully\")\n","bhv_res = np.asarray(bhv_results)\n","np.savetxt(\"drive/MyDrive/Colab Notebooks/dataset/BHV/glove_all_bhv_all_tweets.csv\", bhv_res, delimiter=\",\", fmt='%5.5f')"],"execution_count":null,"outputs":[]}]}